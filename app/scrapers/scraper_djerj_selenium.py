import os
import time
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup

from app import create_app, db
from app.models import DiarioOficial, Publicacao, Advogado

# Configura√ß√£o do Selenium (modo headless)
def iniciar_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")

    driver = webdriver.Chrome(options=chrome_options)
    return driver

def executar_scraper():
    app = create_app()

    # Garantir que o acesso ao banco est√° dentro do contexto do Flask
    with app.app_context():
        hoje = datetime.now().date()
        print(f"üìÖ Rodando scraper para o dia {hoje}")

        # Evitar duplica√ß√£o
        if DiarioOficial.query.filter_by(data_publicacao=hoje).first():
            print(f"üìå Di√°rio Oficial de {hoje} j√° armazenado.")
            return

        driver = iniciar_driver()
        try:
            url = "https://www3.tjrj.jus.br/consultadje/"
            driver.get(url)
            time.sleep(5)  # espera a p√°gina carregar

            # Pega o HTML e analisa
            soup = BeautifulSoup(driver.page_source, "html.parser")

            # Exemplo simples: busca todas as divs de publica√ß√µes
            publicacoes_html = soup.find_all("div", class_="publicacao")

            if not publicacoes_html:
                print("‚ö†Ô∏è Nenhuma publica√ß√£o encontrada no site.")
                return

            # Cria o registro do Di√°rio de hoje
            diario = DiarioOficial(data_publicacao=hoje)
            db.session.add(diario)
            db.session.commit()

            # Percorre e salva publica√ß√µes
            for bloco in publicacoes_html:
                texto = bloco.get_text(separator=" ", strip=True)

                publicacao = Publicacao(
                    diario_id=diario.id,
                    conteudo=texto,
                )
                db.session.add(publicacao)

                # üîé Busca nomes de advogados no conte√∫do
                advogados = Advogado.query.all()
                for adv in advogados:
                    if adv.nome_completo and adv.nome_completo.upper() in texto.upper():
                        print(f"‚úÖ Nome encontrado: {adv.nome_completo}")
                        publicacao.advogados.append(adv)

            db.session.commit()
            print(f"üì¶ Di√°rio de {hoje} salvo com {len(publicacoes_html)} publica√ß√µes.")

        finally:
            driver.quit()


if __name__ == "__main__":
    executar_scraper()
